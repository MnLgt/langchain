{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20deed05",
   "metadata": {},
   "source": [
    "# Unstructured File Loader\n",
    "This notebook covers how to use Unstructured to load files of many types. Unstructured currently supports loading of text files, powerpoints, html, pdfs, images, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2886982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured[local-inference]\n",
      "  Downloading unstructured-0.6.1.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting argilla (from unstructured[local-inference])\n",
      "  Downloading argilla-1.6.0-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2022.12.07 in ./lang/lib/python3.8/site-packages (from unstructured[local-inference]) (2022.12.7)\n",
      "Collecting lxml (from unstructured[local-inference])\n",
      "  Downloading lxml-4.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting markdown (from unstructured[local-inference])\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting msg_parser (from unstructured[local-inference])\n",
      "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk in ./lang/lib/python3.8/site-packages (from unstructured[local-inference]) (3.8.1)\n",
      "Collecting openpyxl (from unstructured[local-inference])\n",
      "  Downloading openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.0/250.0 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in ./lang/lib/python3.8/site-packages (from unstructured[local-inference]) (1.5.3)\n",
      "Requirement already satisfied: pillow in ./lang/lib/python3.8/site-packages (from unstructured[local-inference]) (9.5.0)\n",
      "Collecting pypandoc (from unstructured[local-inference])\n",
      "  Downloading pypandoc-1.11-py3-none-any.whl (20 kB)\n",
      "Collecting python-docx (from unstructured[local-inference])\n",
      "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-magic (from unstructured[local-inference])\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Collecting python-pptx (from unstructured[local-inference])\n",
      "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in ./lang/lib/python3.8/site-packages (from unstructured[local-inference]) (2.28.2)\n",
      "Collecting unstructured-inference>=0.4.2 (from unstructured[local-inference])\n",
      "  Downloading unstructured_inference-0.4.2.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: fastapi in ./lang/lib/python3.8/site-packages (from unstructured-inference>=0.4.2->unstructured[local-inference]) (0.95.1)\n",
      "Requirement already satisfied: huggingface-hub in ./lang/lib/python3.8/site-packages (from unstructured-inference>=0.4.2->unstructured[local-inference]) (0.14.0)\n",
      "Collecting layoutparser[layoutmodels,tesseract] (from unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting onnxruntime (from unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading onnxruntime-1.14.1-cp38-cp38-manylinux_2_27_x86_64.whl (5.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting opencv-python!=4.7.0.68 (from unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-multipart (from unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers in ./lang/lib/python3.8/site-packages (from unstructured-inference>=0.4.2->unstructured[local-inference]) (4.28.1)\n",
      "Requirement already satisfied: uvicorn in ./lang/lib/python3.8/site-packages (from unstructured-inference>=0.4.2->unstructured[local-inference]) (0.21.1)\n",
      "Collecting httpx<0.24,>=0.15 (from argilla->unstructured[local-inference])\n",
      "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting deprecated~=1.2.0 (from argilla->unstructured[local-inference])\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./lang/lib/python3.8/site-packages (from argilla->unstructured[local-inference]) (23.1)\n",
      "Requirement already satisfied: pydantic>=1.7.1 in ./lang/lib/python3.8/site-packages (from argilla->unstructured[local-inference]) (1.10.7)\n",
      "Collecting wrapt<1.15,>=1.13 (from argilla->unstructured[local-inference])\n",
      "  Downloading wrapt-1.14.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy<1.24.0 (from argilla->unstructured[local-inference])\n",
      "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27.0 in ./lang/lib/python3.8/site-packages (from argilla->unstructured[local-inference]) (4.65.0)\n",
      "Requirement already satisfied: backoff in ./lang/lib/python3.8/site-packages (from argilla->unstructured[local-inference]) (2.2.1)\n",
      "Requirement already satisfied: monotonic in ./lang/lib/python3.8/site-packages (from argilla->unstructured[local-inference]) (1.6)\n",
      "Collecting rich<=13.0.1 (from argilla->unstructured[local-inference])\n",
      "  Downloading rich-13.0.1-py3-none-any.whl (238 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.1/238.1 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in ./lang/lib/python3.8/site-packages (from pandas->unstructured[local-inference]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./lang/lib/python3.8/site-packages (from pandas->unstructured[local-inference]) (2023.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./lang/lib/python3.8/site-packages (from markdown->unstructured[local-inference]) (6.6.0)\n",
      "Collecting olefile>=0.46 (from msg_parser->unstructured[local-inference])\n",
      "  Downloading olefile-0.46.zip (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: click in ./lang/lib/python3.8/site-packages (from nltk->unstructured[local-inference]) (8.1.3)\n",
      "Requirement already satisfied: joblib in ./lang/lib/python3.8/site-packages (from nltk->unstructured[local-inference]) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./lang/lib/python3.8/site-packages (from nltk->unstructured[local-inference]) (2023.3.23)\n",
      "Collecting et-xmlfile (from openpyxl->unstructured[local-inference])\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx->unstructured[local-inference])\n",
      "  Downloading XlsxWriter-3.1.0-py3-none-any.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.7/152.7 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in ./lang/lib/python3.8/site-packages (from requests->unstructured[local-inference]) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lang/lib/python3.8/site-packages (from requests->unstructured[local-inference]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./lang/lib/python3.8/site-packages (from requests->unstructured[local-inference]) (1.26.15)\n",
      "Collecting httpcore<0.17.0,>=0.15.0 (from httpx<0.24,>=0.15->argilla->unstructured[local-inference])\n",
      "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3 (from httpx<0.24,>=0.15->argilla->unstructured[local-inference])\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: sniffio in ./lang/lib/python3.8/site-packages (from httpx<0.24,>=0.15->argilla->unstructured[local-inference]) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./lang/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown->unstructured[local-inference]) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./lang/lib/python3.8/site-packages (from pydantic>=1.7.1->argilla->unstructured[local-inference]) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./lang/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->unstructured[local-inference]) (1.16.0)\n",
      "Collecting commonmark<0.10.0,>=0.9.0 (from rich<=13.0.1->argilla->unstructured[local-inference])\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in ./lang/lib/python3.8/site-packages (from rich<=13.0.1->argilla->unstructured[local-inference]) (2.15.1)\n",
      "Requirement already satisfied: starlette<0.27.0,>=0.26.1 in ./lang/lib/python3.8/site-packages (from fastapi->unstructured-inference>=0.4.2->unstructured[local-inference]) (0.26.1)\n",
      "Requirement already satisfied: filelock in ./lang/lib/python3.8/site-packages (from huggingface-hub->unstructured-inference>=0.4.2->unstructured[local-inference]) (3.12.0)\n",
      "Requirement already satisfied: fsspec in ./lang/lib/python3.8/site-packages (from huggingface-hub->unstructured-inference>=0.4.2->unstructured[local-inference]) (2023.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./lang/lib/python3.8/site-packages (from huggingface-hub->unstructured-inference>=0.4.2->unstructured[local-inference]) (6.0)\n",
      "Requirement already satisfied: scipy in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (1.10.1)\n",
      "Collecting iopath (from layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pdfplumber (from layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading pdfplumber-0.9.0-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pdf2image (from layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
      "Collecting pytesseract (from layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: torch in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (2.0.0)\n",
      "Requirement already satisfied: torchvision in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (0.15.1)\n",
      "Collecting effdet (from layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading effdet-0.3.0-py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting coloredlogs (from onnxruntime->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers (from onnxruntime->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: protobuf in ./lang/lib/python3.8/site-packages (from onnxruntime->unstructured-inference>=0.4.2->unstructured[local-inference]) (3.20.3)\n",
      "Requirement already satisfied: sympy in ./lang/lib/python3.8/site-packages (from onnxruntime->unstructured-inference>=0.4.2->unstructured[local-inference]) (1.11.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./lang/lib/python3.8/site-packages (from transformers->unstructured-inference>=0.4.2->unstructured[local-inference]) (0.13.3)\n",
      "Requirement already satisfied: h11>=0.8 in ./lang/lib/python3.8/site-packages (from uvicorn->unstructured-inference>=0.4.2->unstructured[local-inference]) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in ./lang/lib/python3.8/site-packages (from httpcore<0.17.0,>=0.15.0->httpx<0.24,>=0.15->argilla->unstructured[local-inference]) (3.6.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting timm>=0.4.12 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycocotools>=2.0.2 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting omegaconf>=2.0 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (2.0.0)\n",
      "Requirement already satisfied: setuptools in ./lang/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (44.0.0)\n",
      "Requirement already satisfied: wheel in ./lang/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (0.40.0)\n",
      "Requirement already satisfied: cmake in ./lang/lib/python3.8/site-packages (from triton==2.0.0->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (3.26.3)\n",
      "Requirement already satisfied: lit in ./lang/lib/python3.8/site-packages (from triton==2.0.0->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (16.0.2)\n",
      "Collecting portalocker (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pdfminer.six==20221105 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting Wand>=0.6.10 (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading Wand-0.6.11-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.6/143.6 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cryptography>=36.0.0 (from pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading cryptography-40.0.2-cp36-abi3-manylinux_2_28_x86_64.whl (3.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m156.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in ./lang/lib/python3.8/site-packages (from sympy->onnxruntime->unstructured-inference>=0.4.2->unstructured[local-inference]) (1.3.0)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting matplotlib>=2.1.0 (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading matplotlib-3.7.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in ./lang/lib/python3.8/site-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (2.1.2)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading cffi-1.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.7/442.7 kB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading fonttools-4.39.3-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m122.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1 (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources>=3.2.0 in ./lang/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference]) (5.12.0)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference>=0.4.2->unstructured[local-inference])\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: unstructured-inference, python-docx, python-pptx, unstructured, olefile, iopath, antlr4-python3-runtime, pycocotools\n",
      "  Building wheel for unstructured-inference (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unstructured-inference: filename=unstructured_inference-0.4.2-py3-none-any.whl size=35892 sha256=cf761f64ab9dee79ecde7aca847e503fd7803c0f8b61f9de5b7d93e05ddbeba0\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/0d/85/d2/60cee1d421d013b2841ab6c6b3fc55e8efc92290da593601bd\n",
      "  Building wheel for python-docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184601 sha256=3638ae6ceaa92ff291e3020bccc04d24e0124334990129f26a95eb234eec4c0a\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/32/b8/b2/c4c2b95765e615fe139b0b17b5ea7c0e1b6519b0a9ec8fb34d\n",
      "  Building wheel for python-pptx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=471170 sha256=1e3940b6673a66a5cea9dd121d893e79c5e63925658ae9e1d8483e9831c62990\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/b0/38/58/8530ed1681bfee42349acf166867cc9fb369517b2fce83e599\n",
      "  Building wheel for unstructured (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unstructured: filename=unstructured-0.6.1-py3-none-any.whl size=1328915 sha256=c0e9bc3f930072e2b561ef88516c979fb4878d07f238d4df592dc441251bf43e\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/b6/b7/9c/ade000f2795b6690b9315b8c50aa7e714ea61d1e64d60f3f90\n",
      "  Building wheel for olefile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=1b2aafeae4083900cb3aa0e707a0cc3d0004958ec5dfdac09a964a65d73caa96\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/0b/d8/16/1e2d32ad7455728b8af9efdb9d2a0c3d03cd8f2e4be0191b8c\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31539 sha256=bff9c5554ab085a694973b40fdb40d2806344cf8c0ea30581eff8d78049f043a\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/89/3e/24/0f349c0b2eeb6965903035f3b00dbb5c9bea437b4a2f18d82c\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144574 sha256=e191685f1fc347f5ee0651923f0dc6baa13fa807fb30918bd085ad5a64bf3a24\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\n",
      "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp38-cp38-linux_x86_64.whl size=423800 sha256=a0bab5dcf49b96f6e7ae95b33bd97b216291f598150c449cffc4e96fc958566f\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/3e/08/ac/58126fe59992032701437336493f6132e1b72381a62d00b595\n",
      "Successfully built unstructured-inference python-docx python-pptx unstructured olefile iopath antlr4-python3-runtime pycocotools\n",
      "Installing collected packages: Wand, rfc3986, flatbuffers, commonmark, antlr4-python3-runtime, XlsxWriter, wrapt, rich, python-multipart, python-magic, pytesseract, pyparsing, pypandoc, pycparser, portalocker, pdf2image, omegaconf, olefile, numpy, lxml, kiwisolver, humanfriendly, fonttools, et-xmlfile, cycler, python-pptx, python-docx, openpyxl, opencv-python, msg_parser, markdown, iopath, httpcore, deprecated, contourpy, coloredlogs, cffi, onnxruntime, matplotlib, httpx, cryptography, pycocotools, pdfminer.six, argilla, unstructured, pdfplumber, layoutparser, timm, effdet, unstructured-inference\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 13.3.4\n",
      "    Uninstalling rich-13.3.4:\n",
      "      Successfully uninstalled rich-13.3.4\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "Successfully installed Wand-0.6.11 XlsxWriter-3.1.0 antlr4-python3-runtime-4.9.3 argilla-1.6.0 cffi-1.15.1 coloredlogs-15.0.1 commonmark-0.9.1 contourpy-1.0.7 cryptography-40.0.2 cycler-0.11.0 deprecated-1.2.13 effdet-0.3.0 et-xmlfile-1.1.0 flatbuffers-23.3.3 fonttools-4.39.3 httpcore-0.16.3 httpx-0.23.3 humanfriendly-10.0 iopath-0.1.10 kiwisolver-1.4.4 layoutparser-0.3.4 lxml-4.9.2 markdown-3.4.3 matplotlib-3.7.1 msg_parser-1.2.0 numpy-1.23.5 olefile-0.46 omegaconf-2.3.0 onnxruntime-1.14.1 opencv-python-4.7.0.72 openpyxl-3.1.2 pdf2image-1.16.3 pdfminer.six-20221105 pdfplumber-0.9.0 portalocker-2.7.0 pycocotools-2.0.6 pycparser-2.21 pypandoc-1.11 pyparsing-3.0.9 pytesseract-0.3.10 python-docx-0.8.11 python-magic-0.4.27 python-multipart-0.0.6 python-pptx-0.6.21 rfc3986-1.5.0 rich-13.0.1 timm-0.6.13 unstructured-0.6.1 unstructured-inference-0.4.2 wrapt-1.14.1\n",
      "Collecting detectron2@ git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git (to revision v0.6) to /tmp/pip-install-tw1abif4/detectron2_7848bca217284b2094121e844d073416\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-install-tw1abif4/detectron2_7848bca217284b2094121e844d073416\n",
      "  fatal: unable to access 'https://github.com/facebookresearch/detectron2.git/': Failed to connect to github.com port 443: Connection timed out\n",
      "  warning: Clone succeeded, but checkout failed.\n",
      "  You can inspect what was checked out with 'git status'\n",
      "  and retry with 'git restore --source=HEAD :/'\n",
      "\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/facebookresearch/detectron2.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/pip-install-tw1abif4/\u001b[0m\u001b[32mdetectron2_7848bca217284b2094121e844d073416\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/facebookresearch/detectron2.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/pip-install-tw1abif4/\u001b[0m\u001b[32mdetectron2_7848bca217284b2094121e844d073416\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Requirement already satisfied: layoutparser[layoutmodels,tesseract] in ./lang/lib/python3.8/site-packages (0.3.4)\n",
      "Requirement already satisfied: numpy in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (1.23.5)\n",
      "Requirement already satisfied: opencv-python in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (4.7.0.72)\n",
      "Requirement already satisfied: scipy in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (1.10.1)\n",
      "Requirement already satisfied: pandas in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (1.5.3)\n",
      "Requirement already satisfied: pillow in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (9.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (6.0)\n",
      "Requirement already satisfied: iopath in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (0.1.10)\n",
      "Requirement already satisfied: pdfplumber in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (0.9.0)\n",
      "Requirement already satisfied: pdf2image in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (1.16.3)\n",
      "Requirement already satisfied: torch in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (2.0.0)\n",
      "Requirement already satisfied: torchvision in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (0.15.1)\n",
      "Requirement already satisfied: effdet in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (0.3.0)\n",
      "Requirement already satisfied: pytesseract in ./lang/lib/python3.8/site-packages (from layoutparser[layoutmodels,tesseract]) (0.3.10)\n",
      "Requirement already satisfied: timm>=0.4.12 in ./lang/lib/python3.8/site-packages (from effdet->layoutparser[layoutmodels,tesseract]) (0.6.13)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in ./lang/lib/python3.8/site-packages (from effdet->layoutparser[layoutmodels,tesseract]) (2.0.6)\n",
      "Requirement already satisfied: omegaconf>=2.0 in ./lang/lib/python3.8/site-packages (from effdet->layoutparser[layoutmodels,tesseract]) (2.3.0)\n",
      "Requirement already satisfied: filelock in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (4.5.0)\n",
      "Requirement already satisfied: sympy in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (1.11.1)\n",
      "Requirement already satisfied: networkx in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./lang/lib/python3.8/site-packages (from torch->layoutparser[layoutmodels,tesseract]) (2.0.0)\n",
      "Requirement already satisfied: setuptools in ./lang/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->layoutparser[layoutmodels,tesseract]) (44.0.0)\n",
      "Requirement already satisfied: wheel in ./lang/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->layoutparser[layoutmodels,tesseract]) (0.40.0)\n",
      "Requirement already satisfied: cmake in ./lang/lib/python3.8/site-packages (from triton==2.0.0->torch->layoutparser[layoutmodels,tesseract]) (3.26.3)\n",
      "Requirement already satisfied: lit in ./lang/lib/python3.8/site-packages (from triton==2.0.0->torch->layoutparser[layoutmodels,tesseract]) (16.0.2)\n",
      "Requirement already satisfied: portalocker in ./lang/lib/python3.8/site-packages (from iopath->layoutparser[layoutmodels,tesseract]) (2.7.0)\n",
      "Requirement already satisfied: tqdm in ./lang/lib/python3.8/site-packages (from iopath->layoutparser[layoutmodels,tesseract]) (4.65.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./lang/lib/python3.8/site-packages (from pandas->layoutparser[layoutmodels,tesseract]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./lang/lib/python3.8/site-packages (from pandas->layoutparser[layoutmodels,tesseract]) (2023.3)\n",
      "Requirement already satisfied: pdfminer.six==20221105 in ./lang/lib/python3.8/site-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]) (20221105)\n",
      "Requirement already satisfied: Wand>=0.6.10 in ./lang/lib/python3.8/site-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]) (0.6.11)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in ./lang/lib/python3.8/site-packages (from pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]) (3.1.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in ./lang/lib/python3.8/site-packages (from pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]) (40.0.2)\n",
      "Requirement already satisfied: packaging>=21.3 in ./lang/lib/python3.8/site-packages (from pytesseract->layoutparser[layoutmodels,tesseract]) (23.1)\n",
      "Requirement already satisfied: requests in ./lang/lib/python3.8/site-packages (from torchvision->layoutparser[layoutmodels,tesseract]) (2.28.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in ./lang/lib/python3.8/site-packages (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]) (4.9.3)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in ./lang/lib/python3.8/site-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]) (3.7.1)\n",
      "Requirement already satisfied: six>=1.5 in ./lang/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->layoutparser[layoutmodels,tesseract]) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub in ./lang/lib/python3.8/site-packages (from timm>=0.4.12->effdet->layoutparser[layoutmodels,tesseract]) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./lang/lib/python3.8/site-packages (from jinja2->torch->layoutparser[layoutmodels,tesseract]) (2.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./lang/lib/python3.8/site-packages (from requests->torchvision->layoutparser[layoutmodels,tesseract]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./lang/lib/python3.8/site-packages (from requests->torchvision->layoutparser[layoutmodels,tesseract]) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./lang/lib/python3.8/site-packages (from requests->torchvision->layoutparser[layoutmodels,tesseract]) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./lang/lib/python3.8/site-packages (from sympy->torch->layoutparser[layoutmodels,tesseract]) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in ./lang/lib/python3.8/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]) (1.15.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./lang/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in ./lang/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./lang/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./lang/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./lang/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./lang/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]) (5.12.0)\n",
      "Requirement already satisfied: fsspec in ./lang/lib/python3.8/site-packages (from huggingface-hub->timm>=0.4.12->effdet->layoutparser[layoutmodels,tesseract]) (2023.4.0)\n",
      "Requirement already satisfied: pycparser in ./lang/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber->layoutparser[layoutmodels,tesseract]) (2.21)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./lang/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]) (3.15.0)\n"
     ]
    }
   ],
   "source": [
    "# # Install package\n",
    "!pip install \"unstructured[local-inference]\"\n",
    "!pip install \"detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.6#egg=detectron2\"\n",
    "!pip install layoutparser[layoutmodels,tesseract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d62efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install other dependencies\n",
    "# # https://github.com/Unstructured-IO/unstructured/blob/main/docs/source/installing.rst\n",
    "# !brew install libmagic\n",
    "# !brew install poppler\n",
    "# !brew install tesseract\n",
    "# # If parsing xml / html documents:\n",
    "# !brew install libxml2\n",
    "# !brew install libxslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af6a64f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d3e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2593d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"email.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe34e941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee449788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello everyone, please see below for some interesting music law & industry articles I collected from the past two weeks.  If you would like a PDF of the full article, send me an email!\\n\\nNew Music Friday\\n\\nDaniel Caesar – Never Enough\\n\\nEllie Goulding – Higher Than Heaven\\n\\nDrake – Search & Rescue\\n\\nMelanie Martinez – PORTALS\\n\\nYaeji – With A Hammer\\n\\nChildish Gambino Beats 'This Is America' Copyright Su\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7874d01d",
   "metadata": {},
   "source": [
    "## Retain Elements\n",
    "\n",
    "Under the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying `mode=\"elements\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff5b616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"email.txt\", mode=\"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feca3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fec5bbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Hello everyone, please see below for some interesting music law & industry articles I collected from the past two weeks.  If you would like a PDF of the full article, send me an email!', metadata={'source': 'email.txt', 'filename': 'email.txt', 'category': 'NarrativeText'}),\n",
       " Document(page_content='New Music Friday', metadata={'source': 'email.txt', 'filename': 'email.txt', 'category': 'Title'}),\n",
       " Document(page_content='Daniel Caesar – Never Enough', metadata={'source': 'email.txt', 'filename': 'email.txt', 'category': 'Title'}),\n",
       " Document(page_content='Ellie Goulding – Higher Than Heaven', metadata={'source': 'email.txt', 'filename': 'email.txt', 'category': 'Title'}),\n",
       " Document(page_content='Drake – Search & Rescue', metadata={'source': 'email.txt', 'filename': 'email.txt', 'category': 'Title'})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672733fd",
   "metadata": {},
   "source": [
    "## Define a Partitioning Strategy\n",
    "\n",
    "Unstructured document loader allow users to pass in a `strategy` parameter that lets `unstructured` know how to partition the document. Currently supported strategies are `\"hi_res\"` (the default) and `\"fast\"`. Hi res partitioning strategies are more accurate, but take longer to process. Fast strategies partition the document more quickly, but trade-off accuracy. Not all document types have separate hi res and fast partitioning strategies. For those document types, the `strategy` kwarg is ignored. In some cases, the high res strategy will fallback to fast if there is a dependency missing (i.e. a model for document partitioning). You can see how to apply a strategy to an `UnstructuredFileLoader` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767238a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"layout-parser-paper-fast.pdf\", strategy=\"fast\", mode=\"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645f29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60685353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1', lookup_str='', metadata={'source': 'layout-parser-paper-fast.pdf', 'filename': 'layout-parser-paper-fast.pdf', 'page_number': 1, 'category': 'UncategorizedText'}, lookup_index=0),\n",
       " Document(page_content='2', lookup_str='', metadata={'source': 'layout-parser-paper-fast.pdf', 'filename': 'layout-parser-paper-fast.pdf', 'page_number': 1, 'category': 'UncategorizedText'}, lookup_index=0),\n",
       " Document(page_content='0', lookup_str='', metadata={'source': 'layout-parser-paper-fast.pdf', 'filename': 'layout-parser-paper-fast.pdf', 'page_number': 1, 'category': 'UncategorizedText'}, lookup_index=0),\n",
       " Document(page_content='2', lookup_str='', metadata={'source': 'layout-parser-paper-fast.pdf', 'filename': 'layout-parser-paper-fast.pdf', 'page_number': 1, 'category': 'UncategorizedText'}, lookup_index=0),\n",
       " Document(page_content='n', lookup_str='', metadata={'source': 'layout-parser-paper-fast.pdf', 'filename': 'layout-parser-paper-fast.pdf', 'page_number': 1, 'category': 'Title'}, lookup_index=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50728a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 01:14:54.853 INFO    chromadb.telemetry.posthog: Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "2023-04-25 01:14:54.854 INFO    chromadb: Running Chroma using direct local API.\n",
      "2023-04-25 01:14:54.855 WARNING chromadb: Using embedded DuckDB without persistence: data will be transient\n",
      "2023-04-25 01:14:55.409 INFO    chromadb.db.duckdb: Exiting: Cleaning up .chroma directory\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from apikey import apikey \n",
    "\n",
    "import streamlit as st \n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain \n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.utilities import WikipediaAPIWrapper \n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = apikey\n",
    "\n",
    "\n",
    "loader = TextLoader('email.txt', encoding='utf8')\n",
    "loader2 =  TextLoader('email2.txt', encoding='utf8')\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "index = VectorstoreIndexCreator().from_loaders([loader, loader2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "544785cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Yuyen's excuse was that Hotai unintentionally made the videos accessible on YouTube after the music licenses expired, i.e. September 27th, 2013.\n"
     ]
    }
   ],
   "source": [
    "query = \"What was Yuyen's excuse for why his company used the music without a license?\"\n",
    "response = index.query(query, verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc3ba36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-25 01:28:35.748 INFO    unstructured: Reading document from string ...\n",
      "2023-04-25 01:28:35.750 INFO    unstructured: Reading document ...\n",
      "2023-04-25 01:28:36.772 INFO    unstructured: Reading document from string ...\n",
      "2023-04-25 01:28:36.773 INFO    unstructured: Reading document ...\n",
      "2023-04-25 01:28:36.776 INFO    chromadb.telemetry.posthog: Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "2023-04-25 01:28:36.777 INFO    chromadb: Running Chroma using direct local API.\n",
      "2023-04-25 01:28:36.778 WARNING chromadb: Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "ename": "NotEnoughElementsException",
     "evalue": "Number of requested results 4 cannot be greater than number of elements in index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotEnoughElementsException\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m index \u001b[39m=\u001b[39m VectorstoreIndexCreator()\u001b[39m.\u001b[39mfrom_loaders([loader])\n\u001b[1;32m      8\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat is the registration number?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m response \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mquery(query)\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(response)\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/langchain/indexes/vectorstore.py:38\u001b[0m, in \u001b[0;36mVectorStoreIndexWrapper.query\u001b[0;34m(self, question, llm, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m llm \u001b[39m=\u001b[39m llm \u001b[39mor\u001b[39;00m OpenAI(temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     35\u001b[0m chain \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(\n\u001b[1;32m     36\u001b[0m     llm, retriever\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorstore\u001b[39m.\u001b[39mas_retriever(), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m chain\u001b[39m.\u001b[39;49mrun(question)\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/langchain/chains/base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/langchain/chains/base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/langchain/chains/base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    109\u001b[0m     inputs,\n\u001b[1;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/langchain/chains/retrieval_qa/base.py:109\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Run get_relevant_text and llm on input query.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m \u001b[39mIf chain has 'return_source_documents' as 'True', returns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39manswer, docs = res['result'], res['source_documents']\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m question \u001b[39m=\u001b[39m inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key]\n\u001b[0;32m--> 109\u001b[0m docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_docs(question)\n\u001b[1;32m    110\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_documents_chain\u001b[39m.\u001b[39mrun(\n\u001b[1;32m    111\u001b[0m     input_documents\u001b[39m=\u001b[39mdocs, question\u001b[39m=\u001b[39mquestion\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_source_documents:\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/langchain/chains/retrieval_qa/base.py:166\u001b[0m, in \u001b[0;36mRetrievalQA._get_docs\u001b[0;34m(self, question)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_docs\u001b[39m(\u001b[39mself\u001b[39m, question: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretriever\u001b[39m.\u001b[39;49mget_relevant_documents(question)\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/langchain/vectorstores/base.py:279\u001b[0m, in \u001b[0;36mVectorStoreRetriever.get_relevant_documents\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_relevant_documents\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m         docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvectorstore\u001b[39m.\u001b[39;49msimilarity_search(query, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msearch_kwargs)\n\u001b[1;32m    280\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmmr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    281\u001b[0m         docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvectorstore\u001b[39m.\u001b[39mmax_marginal_relevance_search(\n\u001b[1;32m    282\u001b[0m             query, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msearch_kwargs\n\u001b[1;32m    283\u001b[0m         )\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/langchain/vectorstores/chroma.py:144\u001b[0m, in \u001b[0;36mChroma.similarity_search\u001b[0;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimilarity_search\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    129\u001b[0m     query: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    133\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m    134\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \n\u001b[1;32m    136\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39m        List[Document]: List of documents most similar to the query text.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     docs_and_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msimilarity_search_with_score(query, k, \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m)\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m [doc \u001b[39mfor\u001b[39;00m doc, _ \u001b[39min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/langchain/vectorstores/chroma.py:190\u001b[0m, in \u001b[0;36mChroma.similarity_search_with_score\u001b[0;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     query_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function\u001b[39m.\u001b[39membed_query(query)\n\u001b[0;32m--> 190\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_collection\u001b[39m.\u001b[39;49mquery(\n\u001b[1;32m    191\u001b[0m         query_embeddings\u001b[39m=\u001b[39;49m[query_embedding], n_results\u001b[39m=\u001b[39;49mk, where\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m\n\u001b[1;32m    192\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[39mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/chromadb/api/models/Collection.py:202\u001b[0m, in \u001b[0;36mCollection.query\u001b[0;34m(self, query_embeddings, query_texts, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mif\u001b[39;00m where_document \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     where_document \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49m_query(\n\u001b[1;32m    203\u001b[0m     collection_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    204\u001b[0m     query_embeddings\u001b[39m=\u001b[39;49mquery_embeddings,\n\u001b[1;32m    205\u001b[0m     n_results\u001b[39m=\u001b[39;49mn_results,\n\u001b[1;32m    206\u001b[0m     where\u001b[39m=\u001b[39;49mwhere,\n\u001b[1;32m    207\u001b[0m     where_document\u001b[39m=\u001b[39;49mwhere_document,\n\u001b[1;32m    208\u001b[0m     include\u001b[39m=\u001b[39;49minclude,\n\u001b[1;32m    209\u001b[0m )\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/chromadb/api/local.py:247\u001b[0m, in \u001b[0;36mLocalAPI._query\u001b[0;34m(self, collection_name, query_embeddings, n_results, where, where_document, include)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_query\u001b[39m(\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    240\u001b[0m     collection_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m     include: Include \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mdocuments\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmetadatas\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdistances\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     uuids, distances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_db\u001b[39m.\u001b[39;49mget_nearest_neighbors(\n\u001b[1;32m    248\u001b[0m         collection_name\u001b[39m=\u001b[39;49mcollection_name,\n\u001b[1;32m    249\u001b[0m         where\u001b[39m=\u001b[39;49mwhere,\n\u001b[1;32m    250\u001b[0m         where_document\u001b[39m=\u001b[39;49mwhere_document,\n\u001b[1;32m    251\u001b[0m         embeddings\u001b[39m=\u001b[39;49mquery_embeddings,\n\u001b[1;32m    252\u001b[0m         n_results\u001b[39m=\u001b[39;49mn_results,\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    255\u001b[0m     include_embeddings \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m include\n\u001b[1;32m    256\u001b[0m     include_documents \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdocuments\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m include\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/chromadb/db/clickhouse.py:520\u001b[0m, in \u001b[0;36mClickhouse.get_nearest_neighbors\u001b[0;34m(self, where, where_document, embeddings, n_results, collection_name, collection_uuid)\u001b[0m\n\u001b[1;32m    517\u001b[0m     ids \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    519\u001b[0m index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index(collection_uuid)\n\u001b[0;32m--> 520\u001b[0m uuids, distances \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39;49mget_nearest_neighbors(embeddings, n_results, ids)\n\u001b[1;32m    522\u001b[0m \u001b[39mreturn\u001b[39;00m uuids, distances\n",
      "File \u001b[0;32m~/Langchain-Crash-Course/lang/lib/python3.8/site-packages/chromadb/db/index/hnswlib.py:229\u001b[0m, in \u001b[0;36mHnswlib.get_nearest_neighbors\u001b[0;34m(self, query, k, ids)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_dimensionality(query)\n\u001b[1;32m    228\u001b[0m \u001b[39mif\u001b[39;00m k \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_metadata[\u001b[39m\"\u001b[39m\u001b[39melements\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 229\u001b[0m     \u001b[39mraise\u001b[39;00m NotEnoughElementsException(\n\u001b[1;32m    230\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of requested results \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m cannot be greater than number of elements in index \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_metadata[\u001b[39m'\u001b[39m\u001b[39melements\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m     )\n\u001b[1;32m    233\u001b[0m s2 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    234\u001b[0m \u001b[39m# get ids from uuids as a set, if they are available\u001b[39;00m\n",
      "\u001b[0;31mNotEnoughElementsException\u001b[0m: Number of requested results 4 cannot be greater than number of elements in index 1"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "urls = [\n",
    "    \"https://cocatalog.loc.gov/cgi-bin/Pwebrecon.cgi?v1=1&ti=1,1&Search%5FArg=SR0000412664&Search%5FCode=REGS&CNT=25&PID=tJAQFG5-OaIjIT-Elu0DU5tVZOOna&SEQ=20230424212402&SID=1\"]\n",
    "\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "data = loader.load()\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "query = \"What is the registration number?\"\n",
    "response = index.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9ef16",
   "metadata": {},
   "source": [
    "## PDF Example\n",
    "\n",
    "Processing PDF documents works exactly the same way. Unstructured detects the file type and extracts the same types of `elements`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca8a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget  https://raw.githubusercontent.com/Unstructured-IO/unstructured/main/example-docs/layout-parser-paper.pdf -P \"../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686e5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(\"./example_data/layout-parser-paper.pdf\", mode=\"elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f0e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec859d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LayoutParser : A Uniﬁed Toolkit for Deep Learning Based Document Image Analysis', lookup_str='', metadata={'source': '../../layout-parser-paper.pdf'}, lookup_index=0),\n",
       " Document(page_content='Zejiang Shen 1 ( (ea)\\n ), Ruochen Zhang 2 , Melissa Dell 3 , Benjamin Charles Germain Lee 4 , Jacob Carlson 3 , and Weining Li 5', lookup_str='', metadata={'source': '../../layout-parser-paper.pdf'}, lookup_index=0),\n",
       " Document(page_content='Allen Institute for AI shannons@allenai.org', lookup_str='', metadata={'source': '../../layout-parser-paper.pdf'}, lookup_index=0),\n",
       " Document(page_content='Brown University ruochen zhang@brown.edu', lookup_str='', metadata={'source': '../../layout-parser-paper.pdf'}, lookup_index=0),\n",
       " Document(page_content='Harvard University { melissadell,jacob carlson } @fas.harvard.edu', lookup_str='', metadata={'source': '../../layout-parser-paper.pdf'}, lookup_index=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b04cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "lang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
